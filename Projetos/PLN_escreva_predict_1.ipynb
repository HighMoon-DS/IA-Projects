{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRl7OD5PR0tj",
        "outputId": "2c4c4736-f1bb-49e9-fa94-3b8de30084c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.12.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yMEUzvQoOTnm"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from scipy.sparse import hstack\n",
        "import pickle\n",
        "import sys\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar listas de palavras tóxicas e não tóxicas usando pickle\n",
        "with open('toxic_words.pkl', 'rb') as file:\n",
        "    toxic_words_set = pickle.load(file)\n",
        "\n",
        "with open('non_toxic_words.pkl', 'rb') as file:\n",
        "    non_toxic_words_set = pickle.load(file)\n"
      ],
      "metadata": {
        "id": "939TL294SYWG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar vetorizador e modelo\n",
        "\n",
        "tfidf_vectorizer_1 = joblib.load('tfidf_vectorizer.pkl')\n",
        "lg_model_1 = joblib.load('lg_model.pkl')"
      ],
      "metadata": {
        "id": "Q2bX7SmfTbQM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    if text:\n",
        "        return emoji.get_emoji_regexp().sub('', text)\n",
        "    return text\n",
        "\n",
        "# Função para remover caracteres que não são do alfabeto português\n",
        "def remover_chars_n_pt(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "# Funções de pré-processamento\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove links\n",
        "    text = remover_chars_n_pt(text)  # Remove non-Portuguese characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = clean_text(text)\n",
        "    return text\n",
        "\n",
        "def palavras_toxicas(text):\n",
        "    for word in text.split():\n",
        "        if word in toxic_words_set:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def numero_palavras_toxicas(text):\n",
        "    count = 0\n",
        "    for word in text.split():\n",
        "        if word in toxic_words_set:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def palavras_n_toxicas(text):\n",
        "    for word in text.split():\n",
        "        if word in non_toxic_words_set:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def numero_palavras_n_toxicas(text):\n",
        "    count = 0\n",
        "    for word in text.split():\n",
        "        if word in non_toxic_words_set:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def palavras_neutras(text):\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if word in toxic_words_set and word in non_toxic_words_set:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def numero_palavras_neutras(text):\n",
        "    words = text.split()\n",
        "    count = 0\n",
        "    for word in words:\n",
        "        if word in toxic_words_set and word in non_toxic_words_set:\n",
        "            count += 1\n",
        "    return count\n",
        "\n"
      ],
      "metadata": {
        "id": "CJZ6gxEwOZN0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_toxicity(tweet):\n",
        "    # Pré-processar o texto\n",
        "    tweet = preprocess_text(tweet)\n",
        "\n",
        "    # Criar features\n",
        "    features = {\n",
        "        'text': tweet,\n",
        "        'palavras tóxicas': palavras_toxicas(tweet),\n",
        "        'número de palavras tóxicas': numero_palavras_toxicas(tweet),\n",
        "        'palavras não tóxicas': palavras_n_toxicas(tweet),\n",
        "        'número de palavras não tóxicas': numero_palavras_n_toxicas(tweet),\n",
        "        'palavras neutras': palavras_neutras(tweet),\n",
        "        'número de palavras neutras': numero_palavras_neutras(tweet),\n",
        "        'número de palavras': len(tweet),\n",
        "        'número de letras': len(tweet.split())\n",
        "    }\n",
        "     # Transformar texto usando TF-IDF\n",
        "    X_t = tfidf_vectorizer_1.transform([tweet])\n",
        "\n",
        "    # Criar DataFrame com outras features\n",
        "    X_others = pd.DataFrame([features]).drop(columns=['text'])\n",
        "\n",
        "    # Combinar as features\n",
        "    X = hstack([X_t, X_others])\n",
        "\n",
        "    # Fazer a predição\n",
        "    prediction = lg_model_1.predict(X)\n",
        "\n",
        "    return prediction[0]\n"
      ],
      "metadata": {
        "id": "DxRqbmrtT9Lh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Função principal para entrada do usuário\n",
        "if __name__ == \"__main__\":\n",
        "    tweet = input(\"Digite o tweet: \")\n",
        "    prediction = predict_toxicity(tweet)\n",
        "    if prediction == 1:\n",
        "        print(\"\\033[1;31mO tweet é tóxico\\033[0m\")\n",
        "    else:\n",
        "        print(\"\\033[1;32mO tweet é não tóxico\\033[0m\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC8Ypum3UB7y",
        "outputId": "330adcda-4fed-4852-ab52-038e1549638a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite o tweet: vai se fudeeee te amo demais meu amigo\n",
            "\u001b[1;32mO tweet é não tóxico\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}